{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\haria_domzm14\\Downloads\\Spam-Email-Detection-main\\Spam-Email-Detection-main\\email_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# getting dataset details\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  Go until jurong point, crazy.. Available only ...     0\n",
       "1                      Ok lar... Joking wif u oni...     0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3  U dun say so early hor... U c already then say...     0\n",
       "4  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding spam feature\n",
    "df['Spam'] = df['Category'].apply(lambda x:1 if x=='spam' else 0)\n",
    "df = df[['Message', 'Spam']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lowercasing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  go until jurong point, crazy.. available only ...     0\n",
       "1                      ok lar... joking wif u oni...     0\n",
       "2  free entry in 2 a wkly comp to win fa cup fina...     1\n",
       "3  u dun say so early hor... u c already then say...     0\n",
       "4  nah i don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting all words to lowercase\n",
    "df['Message'] = df['Message'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removing Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  go until jurong point crazy available only in ...     0\n",
       "1                            ok lar joking wif u oni     0\n",
       "2  free entry in 2 a wkly comp to win fa cup fina...     1\n",
       "3        u dun say so early hor u c already then say     0\n",
       "4  nah i dont think he goes to usf he lives aroun...     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Message'] = df['Message'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removing Non-Alphabetic Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  go until jurong point crazy available only in ...     0\n",
       "1                            ok lar joking wif u oni     0\n",
       "2  free entry in  a wkly comp to win fa cup final...     1\n",
       "3        u dun say so early hor u c already then say     0\n",
       "4  nah i dont think he goes to usf he lives aroun...     0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Message'] = df['Message'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[free, entry, in, a, wkly, comp, to, win, fa, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  [go, until, jurong, point, crazy, available, o...     0\n",
       "1                     [ok, lar, joking, wif, u, oni]     0\n",
       "2  [free, entry, in, a, wkly, comp, to, win, fa, ...     1\n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...     0\n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...     0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Message'] = df['Message'].str.split()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removing Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  [go, jurong, point, crazy, available, bugis, n...     0\n",
       "1                     [ok, lar, joking, wif, u, oni]     0\n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...     1\n",
       "3      [u, dun, say, early, hor, u, c, already, say]     0\n",
       "4  [nah, dont, think, goes, usf, lives, around, t...     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Message'] = df['Message'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lemmatization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  [go, jurong, point, crazy, available, bugis, n...     0\n",
       "1                     [ok, lar, joking, wif, u, oni]     0\n",
       "2  [free, entry, wkly, comp, win, fa, cup, final,...     1\n",
       "3      [u, dun, say, early, hor, u, c, already, say]     0\n",
       "4  [nah, dont, think, go, usf, life, around, though]     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Message'] = df['Message'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rejoining Words into Sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  Spam\n",
       "0  go jurong point crazy available bugis n great ...     0\n",
       "1                            ok lar joking wif u oni     0\n",
       "2  free entry wkly comp win fa cup final tkts st ...     1\n",
       "3                u dun say early hor u c already say     0\n",
       "4           nah dont think go usf life around though     0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rejoining the sentences\n",
    "df['Message'] = df['Message'].apply(lambda x: ' '.join(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vectorizing the Mails**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 1)\n",
    "X = vectorizer.fit_transform(df['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7476)\t0.19951151603143516\n",
      "  (0, 233)\t0.34991432802526723\n",
      "  (0, 2722)\t0.1640411678130794\n",
      "  (0, 1193)\t0.2956090383151331\n",
      "  (0, 885)\t0.3340310489896538\n",
      "  (0, 3660)\t0.28686783908578817\n",
      "  (0, 7700)\t0.23779337276010057\n",
      "  (0, 2759)\t0.19475744886941995\n",
      "  (0, 887)\t0.2956090383151331\n",
      "  (0, 460)\t0.26608154599358036\n",
      "  (0, 1456)\t0.2709845600501747\n",
      "  (0, 5131)\t0.23892890113851328\n",
      "  (0, 3513)\t0.34991432802526723\n",
      "  (0, 2669)\t0.15482595951135011\n"
     ]
    }
   ],
   "source": [
    "# looking at extracted features\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the output feature\n",
    "y = df['Spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam\n",
       "0    4825\n",
       "1     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam\n",
       "0    4825\n",
       "1    4825\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversampler = RandomOverSampler(random_state = 42)\n",
    "X, y = oversampler.fit_resample(X,y)\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML Model Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra Trees Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1240\n",
      "           1       1.00      1.00      1.00      1173\n",
      "\n",
      "    accuracy                           1.00      2413\n",
      "   macro avg       1.00      1.00      1.00      2413\n",
      "weighted avg       1.00      1.00      1.00      2413\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Creating an instance of Extra Trees Classifier\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "et_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "et_y_pred = et_model.predict(X_test)\n",
    "\n",
    "# Checking score\n",
    "et_accuracy = accuracy_score(y_test, et_y_pred)\n",
    "\n",
    "# Printing Classification Report\n",
    "print(classification_report(y_test, et_y_pred))\n",
    "\n",
    "# Plotting Confusion Matrix (3x3 size)\n",
    "cm = confusion_matrix(y_test, et_y_pred)\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that **`Extra Trees Classifier`** performs the best with very near to `100%` accuracy. We now design a pipeline and then test with a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def pre_process(mail):\n",
    "    # Convert to lowercase\n",
    "    mail = mail.lower()\n",
    "    \n",
    "    # Remove punctuation and non-alphabetic characters using regex\n",
    "    mail = re.sub(r'[^\\w\\s]', '', mail)\n",
    "    mail = re.sub(r'[^a-zA-Z\\s]', '', mail)\n",
    "    \n",
    "    # Tokenize (split the text into words)\n",
    "    mail = mail.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mail = [word for word in mail if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    mail = [lemmatizer.lemmatize(word) for word in mail]\n",
    "    \n",
    "    # Rejoin the tokens into a single string\n",
    "    mail = ' '.join(mail)\n",
    "    \n",
    "    return mail\n",
    "\n",
    "def predict(mail):\n",
    "    # Pre-process the input mail\n",
    "    mail = pre_process(mail)\n",
    "\n",
    "    # Transform the mail using the already fitted vectorizer\n",
    "    mail_vector = vectorizer.transform([mail])  # Note: transform, not fit_transform\n",
    "\n",
    "    # Predict the label using the Extra Trees model\n",
    "    y_pred = et_model.predict(mail_vector)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing Model with Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail: \"Hey, the package you ordered has arrived. I let it in the balcony under the chair. Take it when you return home! rahul\"\n",
      "Prediction: 0\n",
      "\n",
      "Mail: \"Congratulations! You've won a Rs:1000 Amazon gift card. Click here to claim your prize now!\"\n",
      "Prediction: 1\n",
      "\n",
      "Mail: \"Don't forget our meeting at 3 PM today. Looking forward to discussing the new project.\"\n",
      "Prediction: 0\n",
      "\n",
      "Mail: \"You just won a 1 lakh rupees , Tap below to claim now!!!\"\n",
      "Prediction: 1\n",
      "\n",
      "Mail: \"Lunch at 12? Let me know if you're free. We can try that new sushi place.\"\n",
      "Prediction: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample emails\n",
    "mails = [\n",
    "    \"Hey, the package you ordered has arrived. I let it in the balcony under the chair. Take it when you return home! rahul\",\n",
    "    \"Congratulations! You've won a Rs:1000 Amazon gift card. Click here to claim your prize now!\",\n",
    "    \"Don't forget our meeting at 3 PM today. Looking forward to discussing the new project.\",\n",
    "    \"You just won a 1 lakh rupees , Tap below to claim now!!!\",\n",
    "    \"Lunch at 12? Let me know if you're free. We can try that new sushi place.\"\n",
    "]\n",
    "\n",
    "# Predict if each mail is spam or ham\n",
    "for mail in mails:\n",
    "    prediction = predict(mail)\n",
    "    print(f\"Mail: \\\"{mail}\\\"\\nPrediction: {prediction[0]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1422521,
     "sourceId": 2355807,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29907,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
